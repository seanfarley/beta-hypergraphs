#+title: Vectorizing Maximum Likelihood Estimation of $\beta$ models for random hypergraphs
#+date: May 2022
#+options: toc:nil
#+bibliography: report.bib
#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{arrows}
#+latex_header: \usetikzlibrary{matrix}
#+latex_header: \usetikzlibrary{positioning}
#+latex_header: \usetikzlibrary{shapes}
#+latex_header: \newcommand{\prob}{\mathbb{P}}
#+latex_header: \newcommand{\memarray}[1]{$#1_{x}$ \nodepart{two} $#1_{y}$ \nodepart{three} $#1_{z}$ \nodepart{four} $#1_{w}$}

* Introduction
In this report, we'll examine the implementations of the maximum likelihood
estimator (MLE) from Despina et al [cite:@stasi2014bhg]. Motivated by social
network models, Despina et al construct a statistical method for modeling social
interactions. Here we concentrate on the first type of hypergraph called /uniform
hypergraphs/, although initial coding experiments have produced similar speed ups
for /general hypergraphs/.

Recall that the likelihood function [cite:@casella2021statistical] is defined by,

#+name: eq:likelihood
\begin{equation}
L(\theta | {\bf x}) = L(\theta_{1}, \dots, \theta_{k}|x_{1}, \dots, x_{n}) = \prod_{i=1}^{n} f(x_{i}| \theta_{1}, \dots, \theta_{k})
\end{equation}

for some probability distribution function, $f$. We will assume the likelihood
function is differentiable and solve

#+name: eq:mle
\begin{equation}
\frac{\partial}{\partial\theta_{i}} L(\theta | {\bf x}) = 0, \quad i=1, \dots, k
\end{equation}

Our goal in this report is to write an algorithm that calculates the MLE for the
uniform hypergraph. By using vectorization, we can write code that runs on both
the CPU and the GPU.

** Statistical Model for uniform hypergraphs
First, we define a hypergraph $H$ as a pair $(V, F)$ where $\left\{v_{1,} v_{2},
\dots, v_n \right\}$ is a set of /nodes/ and $F$ is a family of non-empty subsets of V
of cardinality different from 1 [cite:@stasi2014bhg]. Define $E=E_n$ as the set
of all realizable edges for a hypergraph on $n$ nodes. We can now write
$x=(V,F)$ as the indicator vector, zero or one, $x = \left\{x_e, e \in E \right\}$
where $x_e = 1$ for $e \in F$ and $x_e \in E \setminus F$.

We use the Bernoulli probability of observing the hypergraph $x$,

#+name: eq:1
\begin{equation}
\prob(x) = \prod_{e \in E} p_e^{x_e} (1 - p_e)^{1 - x_e}
\end{equation}

For /k-uniform/ hypergraphs, where all edges are of size /k/, the probability can be
calculated to be,

#+name: eq:2
\begin{equation}
\prob_\beta(x) = \frac{\exp \left\{ \sum_{e \in {[n] \choose k}} \tilde{\beta_{e}}x_{e}\right\}}{\prod 1 + e^{\tilde{\beta}_{e}}}
= \exp \left\{ \sum_{i \in V} d_i(x)\beta_i - \psi(\beta) \right\}
\end{equation}

where $e \in F$, $\tilde{\beta}_{i} = \sum_{i \in e} \beta_{i}$,  $[n] \choose k$ be the set of
all subsets of size $k$ of the set $\left\{ 1, \dots , n\right\}$, $d_i$ is the
degree of the node $i$, and the normalizing constant is,

#+name: eq:3
\begin{equation}
\psi(\beta) = \sum_{e \in {[n] \choose k}} \log (1 + e^{\tilde{\beta}_{e}}).
\end{equation}

From Despina et al [cite:@stasi2014bhg], it is shown that the /sufficient
statistics for the k-uniform beta model/ are the entries of the degree sequence
vector of the hypergraph, $(d_1(x), \dots, d_{n}(x))$.

** Fixed point algorithm
We will use the fixed point algorithm, with its geometric convergence rate, to
compute the MLE for our beta model [cite:@stasi2014bhg]. From equations [[eq:2]] and
[[eq:3]], we can compute the MLE as,

#+name: eq:4
\begin{equation}
\hat{\beta}_i = \log  d_i - \log \sum_{s \in {[n] \setminus \{i\} \choose k - 1}} \frac{e^{\hat{\tilde{\beta}}_{s}}}{1 + e^{{\hat{\tilde{\beta}}_{s}} + \hat{\beta}_i}}
 := \phi_i(\hat{\beta}).
\end{equation}

The fixed point algorithm is to start with any $\hat{\beta}_{(0)}$ and define
$\hat{\beta}_{(l + 1)} = \phi(\hat{\beta}_{(l)})$  for $l = 0, 1, 2, \dots$.

* Implementation
** Initial =R= code
We will take a look at one particular hot spot of code that will generalize to
For the purposes of this report, we will talk about one particular part of the
original =R= code (but removing some =NaN= checks), namely the part:

#+begin_src R
for (i in 1:n){
  sum.q.beta=0
  for (j in 1:nrow(sets)){
    tuple=sets[j,] # tuple is a (k-1)-tuple of [1,2,...n]
    if (!(i %in% tuple)){
      sum.q.beta = sum.q.beta + prod.exp.beta[j]/(1+(prod.exp.beta[j]*exp.beta[i]))
    }
    beta[i]=log(degrees[i])-log(sum.q.beta)
  }
}
#+end_src

Writing this in a pseudo-=Python= way, we have:

#+caption: Pseudo code for our MLE calculation
#+name: listing:for-loop-python
#+begin_src python
for i in range(rows):
    sum_q = 0
    for j in range(colums):
        if i not in edges_subset:
            sum_q += prod_beta[j] / (1 + prod_beta[j] * exp_beta[i])
        beta[i] = log(degrees[i]) - log(sum_q)
#+end_src

In both cases, =prod_exp_beta= and =exp_beta= are  vectors.

#+caption: Looping accesses each part of the memory one at a time.
#+name: fig:scalar-simd
#+attr_latex: :placement [H]
#+begin_figure
\centering
\begin{tikzpicture}[
  node distance = .5cm,
  mem/.style={
    draw = black,
    thin,
    minimum size = 8mm,
    fill=#1!20,
}]

\foreach \letter [count=\i from 1] in {x, y, z, w} {
    \ifnum\i=1
        \node (A\letter) [mem=yellow] {$A_{\letter}$};
    \else
        \node (A\letter) [mem=yellow, below of=A\prev, anchor=north] {$A_{\letter}$};
    \fi
    \node (plus\letter) [right =of A\letter]  {$+$};
    \node (B\letter) [right =of plus\letter, mem=blue] {$B_{\letter}$};
    \node (eq\letter) [right =of B\letter]  {$=$};
    \node (C\letter) [right =of eq\letter, mem=red] {$C_{\letter}$};
    \xdef\prev{\letter}
}
\end{tikzpicture}
#+end_figure

** Vectorization
For modern processors, chunks of memory can be sent, i.e. vectors, instead of
individual scalars. Figure [[fig:scalar-simd]] can then be written as,

#+caption: Vectorized operation reducing the operations from 4 to 1.
#+name: fig:vector-simd
#+attr_latex: :placement [H]
#+begin_figure
\centering
\begin{tikzpicture}[
  mem/.style={
    draw = black,
    thin,
    minimum size = 8mm,
    fill=#1!20,
    rectangle split,
    rectangle split parts=4,
    rectangle split part align=base,
}]

\node[mem=yellow] (A) {\memarray{A}};

\node (plus) [right of=A]  {$+$};

\node[mem=blue, right of=plus] (B) {\memarray{B}};

\node (eq) [right of=B]  {$=$};

\node[mem=red, right of=eq] (C) {\memarray{C}};
\end{tikzpicture}
#+end_figure

The major difference to take away from this is that this was one operation, as
denoted by the single =+= sign. The list of operations the processor supports
ultimately will depend on the specific hardware. Fortunately, for our MLE
algorithm almost all hardware is supported. By utilizing this, we can refactor
our MLE algorithm to be more efficient.

** Fancy Indexing
Fancy indexing is a term coined by the =NumPy= community that describes taking an
arbitrary subset of an array. For example,

#+begin_src python
>>> import numpy as np
>>> A = np.arange(10) + 3
>>> A
array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12])
>>> indices = np.array([1, 4, 9])
>>> A[indices]
array([ 4,  7, 12])
#+end_src

A key takeaway from this code snippet should be that no memory was copied just a
special "view" of a subset of our original array. This will be an efficient way
to avoid using an =if= statement in our computationally expensive core. From our
original python psuedo-code, the strategy will be to calculate the indices
/before/ vectorizing.

#+begin_src python
indices = []
for i in range(n):
    for j in range(m):
        if i not in degrees_choose_k:
            indices.append(j)
#+end_src

#+caption: A "fancy indexing" of an array, which in =Python= is written as =a[0, 2, 5, 6]=
#+name: fig:fancy-index
#+attr_latex: :placement [H]
#+begin_figure
\centering
\begin{tikzpicture}
  \matrix (memory) [
    matrix of nodes,
    nodes={
        rectangle, draw=black, minimum height=2.25em, minimum width=2.25em,
        anchor=center,
        fill=blue!40,
    }
  ] {
    $a_{0}$ & |[fill=red!40]| $a_{1}$ & $a_{2}$ &  |[fill=red!40]| $a_{3}$ &  |[fill=red!40]| $a_{4}$ & $a_{5}$ & $a_{6}$ &  |[fill=red!40]| $a_{7}$\\
  };

\matrix (sub) [
below=1cm of memory,
matrix of nodes,
nodes={
        rectangle, draw=black, minimum height=2.25em, minimum width=2.25em,
        anchor=center,
        fill=blue!40,
    }
] {
    $a_{0}$ & $a_{2}$ & $a_{5}$ & $a_{6}$ \\
};

\draw[->] (memory-1-1.south) to[out=240, in=90] (sub-1-1.north);
\draw[->] (memory-1-3) to[out=260, in=90] (sub-1-2);
\draw[->] (memory-1-6) to[out=280, in=90] (sub-1-3);
\draw[->] (memory-1-7) to[out=300, in=90] (sub-1-4);
\end{tikzpicture}
#+end_figure

Note that the code in our repository is written in a slightly different way; see
the section [[*List comprehension][List comprehension]] for more detail.

*** Removing the column loop
We now have an index array. For a graph of size $5 \choose 2$ this is,

#+begin_src python
[[4 5 6 7 8 9]
 [1 2 3 7 8 9]
 [0 2 3 5 6 9]
 [0 1 3 4 6 8]
 [0 1 2 4 5 7]]
#+end_src

Following from Listing [[listing:for-loop-python]], our new vectorized algorithm is now,

#+begin_src python
for i in range(rows):
    ind = indices[i]
    sum_q = sum(prod_beta[ind] / (1 + prod_beta[ind] * exp_beta[i]))
    beta[i] = log(degrees[i]) - log(sum_q)
#+end_src

We no longer need to have the inner-loop (column) where we use an =if= statement
to test for inclusion. That logic is done a priori (and much faster). For a
hypergraph of $n=25, k=3$,

#+begin_src shell
$ python3 beta_hypergraphs.py
Running R-converted (for-loops) code (with n=25, k=3)
beta_fixed_point_R took 257.1152 seconds
Running python vectorized code (with n=25, k=3)
beta_fixed_point took 0.4320 seconds
#+end_src

This is /quite/ the speed up.

In Figure [[fig:first-pass-vec-op]], we can see in a visual way how this first pass
is vectorized and sent to the processor as a chunk of memory to perform just a
few operations.

#+caption: Vectorization of computing =sum_q= where =pb= is =prod_beta= and =eb= is =exp_beta=
#+name: fig:first-pass-vec-op
#+attr_latex: :placement [H]
#+begin_figure
\centering
\begin{tikzpicture}[
memsize/.style = {
rectangle,
minimum height=2.25em,
 minimum width=2.25em,
},
mem/.style = {
    matrix of nodes,
    nodes={
        draw=black,
        memsize,
        anchor=center,
        fill=#1!40,
    },
}
]
\matrix[mem=green] (X) {
    $x_0$ \\
    $x_1$ \\
    $x_2$ \\
    $x_3$ \\
    $x_4$ \\
  };

\node[memsize, right of=X] (div) {$\div$};

\matrix[mem=blue, right=5mm of div, left delimiter=(,] (Y) {
    $y_0$ \\
    $y_1$ \\
    $y_2$ \\
    $y_3$ \\
    $y_4$ \\
  };

\node[memsize, right of=Y] (times) {$*$};

\matrix[mem=red, right of=times] (Z) {
    $z_0$ \\
    $z_1$ \\
    $z_2$ \\
    $z_3$ \\
    $z_4$ \\
  };

\node[memsize, right of=Z] (plus) {$+$};

\matrix[mem=purple, right of=plus, right delimiter=), nodes in empty cells] (ones) {
     1 \\
     \\
     \\
     \\
     \\
  };

\draw[->, very thick, purple!70] (ones-1-1) -- (ones-5-1.center);
\draw (ones-3-1.east) node[fill=purple!5,transform shape, rotate=-90] {broadcast};

\node[memsize, right=5mm of ones] (equals) {$=$};

\matrix[mem=yellow, right of=equals, right delimiter=\}] (result) {
    $r_0$ \\
    $r_1$ \\
    $r_2$ \\
    $r_3$ \\
    $r_4$ \\
  };

\draw (result-2-1.east) node[fill=yellow!5,transform shape, rotate=-90, yshift=9mm, xshift=8mm] {reduce by sum};

\node[memsize, right=12mm of result] (reduced_eq) {$=$};
\node[memsize, fill=yellow!40, right=1mm of reduced_eq] (reduced) {$r$};
\end{tikzpicture}
#+end_figure

This is the /crux/ of how we speed up this calculation.

*** Removing the row loop
The astute reader will notice that this for-loop is over a matrix and operates
row-by-row. Therefore, we can further vectorize this by just writing it as a
matrix-vector operation,

#+begin_src python
pb = prod_beta[indices]
sum_q = sum(pb / (1 + (pb.T * exp_beta).T), axis=1)

beta = log(degrees) - log(sum_q)
#+end_src

where =T= denotes the transpose, which is needed for the correct shape of
matrix-vector multiplication.

#+begin_src shell
$ python3 beta_hypergraphs.py
Running python vectorized code (with n=25, k=3)
beta_fixed_point took 0.4059 seconds
#+end_src

This is another speed-up but not as great as before. Quite normal as the
algorithm is queuing rows and rows of data into the cache which is almost
equivalent to doing a for-loop over the rows.

*** GPU speedup
A great, in fact one of the best, benefits to writing algorithms in a vectorized
way is that it can seamlessly be run on a GPU. By using CuPy
[cite:@cupy_learningsys2017], we can experiment with running on the GPU.

#+TODO: [H] didn't work for begin_figure

#+name: fig:cpg-gpu
#+caption: GPUs offer a consider speedup over traditional CPU cores
\begin{figure}[H]
\centering

\begin{tikzpicture}
  \matrix[
    draw,
    matrix of nodes,
    row sep=.1cm,
    column sep=.1cm,
    nodes={
        rectangle, draw=black, minimum height=2.25em, minimum width=2.25em,
        anchor=center, %align=center, %text width=2em,
        fill=blue!40,
    }
  ] (CPU) {
    Core 1 & Core 3 \\
    Core 2 & Core 4 \\
  };

  \node[
      above=3mm of CPU,
      anchor=south,
      text width=3cm,
      align=center
  ] {
    CPU\\ with four cores
  };

  \matrix[
    draw,
    right=1cm of CPU,
    matrix of nodes,
    row sep=.1cm,
    column sep=.1cm,
    nodes in empty cells,
    nodes={
        rectangle, draw=black, %minimum height=2.25em, minimum width=2.25em,
        anchor=center, %align=center, %text width=2em,
        fill=red!40,
    }
  ] (GPU) {
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
     & & & & & & & & & & & & & & & & \\
  };

\node[
      above=3mm of GPU,
      anchor=south,
      text width=6cm,
      align=center
  ] {
    GPU\\ with hundreds of cores
  };
\end{tikzpicture}
\end{figure}

#+begin_src shell
$ python3 beta_hypergraphs_cupy.py
Running cupy vectorized code (with n=25, k=3)
beta_fixed_point took 1.4462 seconds
#+end_src

This is =1s= slower than running the same algorithm on the CPU. This is a common
situation with using GPUs. The reason is that there is an unavoidable overhead
to transferring memory from the CPU to the GPU.

To see what kind of speed up the GPU gives us, we need to increase the size of the
problem. We set the size of $k=6$ for a size of $25 \choose 5$,

#+begin_src shell
$ python3 beta_hypergraphs.py
Running R-converted (for-loops) code (with n=25, k=6)
beta_fixed_point_R took *didn't complete after waiting two hours*

Running python vectorized code (with n=25, k=6)
beta_fixed_point took 18.9432 seconds

$ python3 beta_hypergraphs_cupy.py
Running cupy vectorized code (with n=25, k=6)
beta_fixed_point took 1.7377 seconds
#+end_src

With this problem size, we can see the massive speedup that the GPU gave us.

* Miscellanea
** List comprehension
For a bit faster performance, we can use a =Python= technique known as list
comprehension,

#+begin_src python
indices = [[j for j in range(m) if i not in degrees_choose_k[j]]
           for i in range(n)]
#+end_src

For a set the size of $25 \choose 5$ , we have the timings,

#+begin_src shell
$ python3 -m timeit -s 'import itertools; n=25; k=6; ind=[]; sets=list(itertools.combinations(range(n), k-1));' '
for i in range(n):
    for j in range(len(sets)):
        if i not in sets[j]:
            ind.append(j)
'
2 loops, best of 5: 143 msec per loop
$ python3 -m timeit -s 'import itertools; n=25; k=6; ind=[]; sets=list(itertools.combinations(range(n), k-1));' '[[j for j in range(len(sets)) if i not in sets[j]] for i in range(n)]'
2 loops, best of 5: 135 msec per loop
#+end_src

So, this boost is almost negligible but it is common practice in =Python= to write
this is in a list comprehension way.

** Numba JIT
Just-in-time (JIT) compilation is a method of compiling code during the execution of a
program rather than before. This allows traditionally interpreted computer
languages, such as =Python=, to gain some speed boosts that compiled languages,
such as =C++=, have.

To try this method in =Python=, we will use =Numba= [cite:@10.1145/2833157.2833162],
a library for =Python= that provides a JIT compiler. This approach is best suited
for non-vectorized code such as Listing [[listing:for-loop-python]].

#+begin_src shell
$ python3 beta_hypergraphs.py
Running python vectorized code (with n=25, k=6)
beta_fixed_point took 18.9432 seconds

Running python jit'd code (with n=25, k=6)
beta_fixed_point_R jit'd took 24.4585 seconds
#+end_src

This is a considerable speedup for not changing the original algorithm of
Listing [[listing:for-loop-python]].

#+print_bibliography:
