#+title: Maximum Likelihood Estimation of $\beta$ models for random hypergraphs
#+date: Tuesday, May 3, 2022
#+options: toc:nil
#+bibliography: report.bib
#+latex_header: \usepackage{tikz}
#+latex_header: \newcommand{\prob}{\mathbb{P}}

#+begin_export latex
\definecolor{purple}{rgb}{0.5, 0.0, 0.5}
\newenvironment{todo}{{\bf TODO:} \sf \begingroup\color{purple}}{\endgroup}
#+end_export


* Introduction
In this report, we'll examine the implementations of the maximum likelihood
estimator (MLE) from [cite:@https://doi.org/10.48550/arxiv.1407.1004]. Motivated
by social network models, Despina et al construct a statistical method for
modeling social interactions. Here we concentrate on the first type of
hypergraph called /uniform hypergraphs/, although initial coding experiments have
produced similar speed ups for /general hypergraphs/.

Our goal in this report is to write an algorithm that calculates the MLE for the
uniform hypergraph. By using vectorization, we can write code that runs on both
the CPU and the GPU.

** Statistical Model for uniform hypergraphs
First, we define a hypergraph $H$ as a pair $(V, F)$ where $\left\{v_{1,} v_{2},
\dots, v_n \right\}$ is a set of /nodes/ and $F$ is a family of non-empty subsets of V
of cardinality different from 1
[cite:@https://doi.org/10.48550/arxiv.1407.1004]. Define $E=E_n$ as the set of
all realized edges for a hypergraph on $n$ nodes. We can now write $x=(V,F)$ as
the indicator vector, zero or one, $x = \left\{x_e, e \in E \right\}$ where $x_e
= 1$ for $e \in F$ and $x_e \in E \setminus F$.

We use the Bernoulli probability of observing the hypergraph $x$,

#+name: eq:1
\begin{equation}
\prob(x) = \prod_{e \in E} p_e^{x_e} (1 - p_e)^{1 - x_e}
\end{equation}

For /k-uniform/ hypergraphs, where all edges are of size /k/, the probability can be
calculated to be,

#+name: eq:2
\begin{equation}
\prob_\beta(x) = \frac{\exp \left\{ \sum_{e \in {[n] \choose k}} \tilde{\beta_{e}}x_{e}\right\}}{\prod 1 + e^{\tilde{\beta}_{e}}}
= \exp \left\{ \sum_{i \in V} d_i(x)\beta_i - \psi(\beta) \right\}
\end{equation}

where $e \in F$, $\tilde{\beta}_{i} = \sum_{i \in e} \beta_{i}$,  $[n] \choose k$ be the set of
all subsets of size $k$ of the set $\left\{ 1, \dots , n\right\}$, $d_i$ is the
degree of the node $i$, and the normalizing constant is,

#+name: eq:3
\begin{equation}
\psi(\beta) = \sum_{e \in {[n] \choose k}} \log (1 + e^{\tilde{\beta}_{e}}).
\end{equation}

From [cite:@https://doi.org/10.48550/arxiv.1407.1004], it is shown that the
/sufficient statistics for the k-uniform beta model/ are the entries of the
degree sequence vector of the hypergraph, $(d_1(x), \dots, d_{n}(x))$.

** Fixed point algorithm
We will use the fixed point algorithm, with its geometric convergence rate, to
compute the MLE for our beta model
[cite:@https://doi.org/10.48550/arxiv.1407.1004]. From equations [[eq:2]] and [[eq:3]],
we can compute the MLE as,

#+name: eq:4
\begin{equation}
\hat{\beta}_i = \log  d_i - \log \sum_{s \in {[n] \setminus \{i\} \choose k - 1}} \frac{e^{\hat{\tilde{\beta}}_{s}}}{1 + e^{{\hat{\tilde{\beta}}_{s}} + \hat{\beta}_i}}
 := \phi_i(\hat{\beta}).
\end{equation}

The fixed point algorithm is to start with any $\hat{\beta}_{(0)}$ and define
$\hat{\beta}_{(l + 1)} = \phi(\hat{\beta}_{(l)})$  for $l = 0, 1, 2, \dots$.

* Implementation
** Initial =R= code
We will take a look at one particular hot spot of code that will generalize to
For the purposes of this report, we will talk about one particular part of the
original =R= code (but removing some =NaN= checks), namely the part:

#+begin_src R
for (i in 1:n){
  sum.q.beta=0
  for (j in 1:nrow(sets)){
    tuple=sets[j,] # tuple is a (k-1)-tuple of [1,2,...n]
    if (!(i %in% tuple)){
      sum.q.beta = sum.q.beta + prod.exp.beta[j]/(1+(prod.exp.beta[j]*exp.beta[i]))
    }
    beta[i]=log(degrees[i])-log(sum.q.beta)
  }
}
#+end_src

Writing this in a pseudo-=Python= way, we have:
\linebreak

#+caption: Pseudo code for our MLE calculation
#+name: listing:for-loop-python
#+begin_src python
for i in range(rows):
    sum_q = 0
    for j in range(colums):
        if i not in edges_subset:
            sum_q += prod_beta[j] / (1 + prod_beta[j] * exp_beta[i])
        beta[i] = log(degrees[i]) - log(sum_q)
#+end_src

In both cases, =prod_exp_beta= and =exp_beta= are  vectors.

#+caption: Looping accesses each part of the memory one at a time.
#+name: fig:scalar-simd
#+attr_latex: :placement [H]
#+attr_latex: :width 0.4\linewidth
[[./scalar-simd.png]]

** Vectorization
For modern processors, chunks of memory can be sent, i.e. vectors, instead of
individual scalars. Figure [[fig:scalar-simd]] can then be written as,

#+caption: Vectorized operation of [[fig:scalar-simd]], reducing the operations from 4 to 1.
#+name: fig:vector-simd
#+attr_latex: :placement [H]
#+attr_latex: :width 0.4\linewidth
[[./vector-simd.png]]

The major difference to take away from this is that this was one operation, as
denoted by the single =+= sign. The list of operations the processor supports
ultimately will depend on the specific hardware. Fortunately, for our MLE
algorithm almost all hardware is supported. By utilizing this, we can refactor
our MLE algorithm to be more efficient.

** Fancy Indexing
Fancy indexing is a term coined by the =NumPy= community that describes taking an
arbitrary subset of an array. For example,

#+begin_src python
>>> import numpy as np
>>> A = np.arange(10) + 3
>>> A
array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12])
>>> indices = np.array([1, 4, 9])
>>> A[indices]
array([ 4,  7, 12])
#+end_src

A key takeaway from this code snippet should be that no memory was copied just a
special "view" of a subset of our original array. This will be an efficient way
to avoid using an =if= statement in our computationally expensive core. From our
original python psuedo-code, the strategy will be to calculate the indices
/before/ vectorizing.

#+begin_src python
indices = []
for i in range(n):
    for j in range(m):
        if i not in degrees_choose_k:
            indices.append(j)
#+end_src

#+caption: A "fancy indexing" of an array
#+name: fig:fancy-index
[[./fancy-index.pdf]]

Note that the code in our repository is written in a slightly different way; see
the section [[*List comprehension][List comprehension]] for more detail.

*** Removing the column loop
We now have an index array. For a graph of size $5 \choose 2$ this is,

#+begin_src python
[[4 5 6 7 8 9]
 [1 2 3 7 8 9]
 [0 2 3 5 6 9]
 [0 1 3 4 6 8]
 [0 1 2 4 5 7]]
#+end_src

Following from Listing [[listing:for-loop-python]], our new vectorized algorithm is now,

#+begin_src python
for i in range(rows):
    ind = indices[i]
    sum_q = sum(prod_beta[ind] / (1 + prod_beta[ind] * exp_beta[i]))
    beta[i] = log(degrees[i]) - log(sum_q)
#+end_src

We no longer need to have the inner-loop (column) where we use an =if= statement
to test for inclusion. That logic is done a priori (and much faster).

#+begin_todo
Insert timings for first pass at vectorization
#+end_todo

In Figure [[fig:first-pass-vec-op]], we can see in a visual way how this first pass
is vectorized and sent to the processor as a chunk of memory to perform just a
few operations.

#+caption: Vectorization of computing =sum_q= where =pb= is =prod_beta= and =eb= is =exp_beta=
#+name: fig:first-pass-vec-op
#+attr_latex: :placement [H]
[[./first-pass-vec-op.pdf]]

This is the /crux/ of how we speed up this calculation.

*** Removing the row loop
The astute reader will notice that this for-loop is over a matrix and operates
row-by-row. Therefore, we can further vectorize this by just writing it as a
matrix-vector operation,

#+begin_src python
pb = prod_beta[indices]
sum_q = sum(pb / (1 + (pb.T * exp_beta).T), axis=1)

beta = log(degrees) - log(sum_q)
#+end_src

where =T= denotes the transpose, which is needed for the correct shape of
matrix-vector multiplication.

#+begin_todo
Insert timings for second pass at vectorization
#+end_todo

* Miscellanea
** List comprehension
For a bit faster performance, we can use a =Python= technique known as list
comprehension,

#+begin_src python
indices = [[j for j in range(m) if i not in degrees_choose_k[j]]
           for i in range(n)]
#+end_src

For a set the size of $25 \choose 5$ , we have the timings,

#+begin_src shell
 python3 -m timeit -s 'import itertools; n=25; k=6; ind=[]; sets=list(itertools.combinations(range(n), k-1));' '
for i in range(n):
    for j in range(len(sets)):
        if i not in sets[j]:
            ind.append(j)
'
2 loops, best of 5: 143 msec per loop
 python3 -m timeit -s 'import itertools; n=25; k=6; ind=[]; sets=list(itertools.combinations(range(n), k-1));' '[[j for j in range(len(sets)) if i not in sets[j]] for i in range(n)]'
2 loops, best of 5: 135 msec per loop
#+end_src

So, this boost is almost negligible but it is common practice in =Python= to write
this is in a list comprehension way.

** Numba JIT
Just-in-time (JIT) compilation is a method of compiling code during the execution of a
program rather than before. This allows traditionally interpreted computer
languages, such as =Python=, to gain some speed boosts that compiled languages,
such as =C++=, have.

To try this method in =Python=, we will use =Numba= [cite:@10.1145/2833157.2833162],
a library for =Python= that provides a JIT compiler. This approach is best suited
for non-vectorized code such as Listing [[listing:for-loop-python]].

#+begin_todo
insert jit timings
#+end_todo

** Alternatives to fancy indexing
#+begin_todo
write about non-subset method using identity (i.e. 0 or 1)
#+end_todo

#+begin_todo
insert diagram showing non-subset memory
#+end_todo

#+print_bibliography:
